{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION - SEQUENTIAL BACKWARD SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVERVIEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [INTRODUCTION](#INTRODUCTION)  \n",
    "- [FEATURE SELECTION](#FEATURE-SELECTION)\n",
    "    - [SEQUENTIAL BACKWARD SELECTION](#SEQUENTIAL-BACKWARD-SELECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCTION\n",
    "\n",
    "A way to reduce overfitting (complexity of the model) is **Dimensionality Reduction** via **feature selection**. There are two main categories:\n",
    "- **Feature Selection**\n",
    "- **Feature Extraction**\n",
    "\n",
    "In case of **feature selection** a subset of the original features is used in order to build our model. On the other hand, in **feature extraction** the objective is to create a new reduced feature set using the information from the original feature set. As stated before, both tecniques are useful to deal with the *[curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)*.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "<strong>Curse of Dimensionality</strong>\n",
    "<br/>\n",
    "As additional variables are added to a model, it may be able to predict a number better in regresssion models or discriminate better between classes in a classification model. The problem is that convergence on those solutions during either the error minimization process or the iterative learning process gets increasingly slow as additional features are added to the analysis.\n",
    "</div>\n",
    "\n",
    "\n",
    "## FEATURE SELECTION\n",
    "\n",
    "The motivation behind feaure selection is to **select the best subset of features that are more relevant to the problem**. By doing this, we can:\n",
    "\n",
    "- Reduce overfitting/complexity (reduce the generalization error)\n",
    "- Improve computational efficiency\n",
    "- Enhance data quality\n",
    "- Make the results more understandable\n",
    "\n",
    "This set of tecniques/algorithms can be useful for algorithms in which regularization is not supported (KNN, SVM, Naive Bayes, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNIVARIATE FEATURE SELECTION\n",
    "\n",
    "**Univariate feature selection** works by selecting the best features based on univariate statistical tests.\n",
    "\n",
    "For instance, we can perfom $\\chi^2$ test to the samples to retrieve only the best features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', \n",
    "                      header=None)\n",
    "df_wine.columns = ['Class label', \n",
    "                   'Alcohol', \n",
    "                   'Malic acid', \n",
    "                   'Ash', \n",
    "                   'Alcalinity of ash', \n",
    "                   'Magnesium', \n",
    "                   'Total phenols', \n",
    "                   'Flavanoids', \n",
    "                   'Nonflavanoid phenols', \n",
    "                   'Proanthocyanins', \n",
    "                   'Color intensity', \n",
    "                   'Hue', \n",
    "                   'OD280/OD315 of diluted wines', \n",
    "                   'Proline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class label</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class label  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0            1    14.23        1.71  2.43               15.6        127   \n",
       "1            1    13.20        1.78  2.14               11.2        100   \n",
       "2            1    13.16        2.36  2.67               18.6        101   \n",
       "3            1    14.37        1.95  2.50               16.8        113   \n",
       "4            1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class label</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.938202</td>\n",
       "      <td>13.000618</td>\n",
       "      <td>2.336348</td>\n",
       "      <td>2.366517</td>\n",
       "      <td>19.494944</td>\n",
       "      <td>99.741573</td>\n",
       "      <td>2.295112</td>\n",
       "      <td>2.029270</td>\n",
       "      <td>0.361854</td>\n",
       "      <td>1.590899</td>\n",
       "      <td>5.058090</td>\n",
       "      <td>0.957449</td>\n",
       "      <td>2.611685</td>\n",
       "      <td>746.893258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.775035</td>\n",
       "      <td>0.811827</td>\n",
       "      <td>1.117146</td>\n",
       "      <td>0.274344</td>\n",
       "      <td>3.339564</td>\n",
       "      <td>14.282484</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.998859</td>\n",
       "      <td>0.124453</td>\n",
       "      <td>0.572359</td>\n",
       "      <td>2.318286</td>\n",
       "      <td>0.228572</td>\n",
       "      <td>0.709990</td>\n",
       "      <td>314.907474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.030000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>278.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.362500</td>\n",
       "      <td>1.602500</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>1.742500</td>\n",
       "      <td>1.205000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>3.220000</td>\n",
       "      <td>0.782500</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>500.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>13.050000</td>\n",
       "      <td>1.865000</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>2.355000</td>\n",
       "      <td>2.135000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.555000</td>\n",
       "      <td>4.690000</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>673.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>13.677500</td>\n",
       "      <td>3.082500</td>\n",
       "      <td>2.557500</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>3.170000</td>\n",
       "      <td>985.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>14.830000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.230000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>3.880000</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>3.580000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.710000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1680.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Class label     Alcohol  Malic acid         Ash  Alcalinity of ash  \\\n",
       "count   178.000000  178.000000  178.000000  178.000000         178.000000   \n",
       "mean      1.938202   13.000618    2.336348    2.366517          19.494944   \n",
       "std       0.775035    0.811827    1.117146    0.274344           3.339564   \n",
       "min       1.000000   11.030000    0.740000    1.360000          10.600000   \n",
       "25%       1.000000   12.362500    1.602500    2.210000          17.200000   \n",
       "50%       2.000000   13.050000    1.865000    2.360000          19.500000   \n",
       "75%       3.000000   13.677500    3.082500    2.557500          21.500000   \n",
       "max       3.000000   14.830000    5.800000    3.230000          30.000000   \n",
       "\n",
       "        Magnesium  Total phenols  Flavanoids  Nonflavanoid phenols  \\\n",
       "count  178.000000     178.000000  178.000000            178.000000   \n",
       "mean    99.741573       2.295112    2.029270              0.361854   \n",
       "std     14.282484       0.625851    0.998859              0.124453   \n",
       "min     70.000000       0.980000    0.340000              0.130000   \n",
       "25%     88.000000       1.742500    1.205000              0.270000   \n",
       "50%     98.000000       2.355000    2.135000              0.340000   \n",
       "75%    107.000000       2.800000    2.875000              0.437500   \n",
       "max    162.000000       3.880000    5.080000              0.660000   \n",
       "\n",
       "       Proanthocyanins  Color intensity         Hue  \\\n",
       "count       178.000000       178.000000  178.000000   \n",
       "mean          1.590899         5.058090    0.957449   \n",
       "std           0.572359         2.318286    0.228572   \n",
       "min           0.410000         1.280000    0.480000   \n",
       "25%           1.250000         3.220000    0.782500   \n",
       "50%           1.555000         4.690000    0.965000   \n",
       "75%           1.950000         6.200000    1.120000   \n",
       "max           3.580000        13.000000    1.710000   \n",
       "\n",
       "       OD280/OD315 of diluted wines      Proline  \n",
       "count                    178.000000   178.000000  \n",
       "mean                       2.611685   746.893258  \n",
       "std                        0.709990   314.907474  \n",
       "min                        1.270000   278.000000  \n",
       "25%                        1.937500   500.500000  \n",
       "50%                        2.780000   673.500000  \n",
       "75%                        3.170000   985.000000  \n",
       "max                        4.000000  1680.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class label                     0\n",
       "Alcohol                         0\n",
       "Malic acid                      0\n",
       "Ash                             0\n",
       "Alcalinity of ash               0\n",
       "Magnesium                       0\n",
       "Total phenols                   0\n",
       "Flavanoids                      0\n",
       "Nonflavanoid phenols            0\n",
       "Proanthocyanins                 0\n",
       "Color intensity                 0\n",
       "Hue                             0\n",
       "OD280/OD315 of diluted wines    0\n",
       "Proline                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class label                       int64\n",
       "Alcohol                         float64\n",
       "Malic acid                      float64\n",
       "Ash                             float64\n",
       "Alcalinity of ash               float64\n",
       "Magnesium                         int64\n",
       "Total phenols                   float64\n",
       "Flavanoids                      float64\n",
       "Nonflavanoid phenols            float64\n",
       "Proanthocyanins                 float64\n",
       "Color intensity                 float64\n",
       "Hue                             float64\n",
       "OD280/OD315 of diluted wines    float64\n",
       "Proline                           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('reduce_dim', SelectKBest(k=10, score_func=<function chi2 at 0x11c81bea0>)), ('classify', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'reduce_dim__k': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "k_set = [i+1 for i in range(X_train.shape[1]-1)]\n",
    "\n",
    "pipe = Pipeline([('reduce_dim', SelectKBest(chi2)),\n",
    "                 ('classify', KNeighborsClassifier(n_neighbors=5))])\n",
    "\n",
    "parameters = {'reduce_dim__k':k_set}\n",
    "grid = GridSearchCV(estimator=pipe, \n",
    "                    param_grid=parameters, \n",
    "                    cv=5, \n",
    "                    n_jobs=-1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('reduce_dim', SelectKBest(k=2, score_func=<function chi2 at 0x11c81bea0>)), ('classify', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'))])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66935483870967738"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 0.02331686,  0.00423059,  0.00748839,  0.00458665,  0.00333595,\n",
       "         0.00390344,  0.00363383,  0.0034132 ,  0.0032104 ,  0.00346246,\n",
       "         0.00324221,  0.00455141]),\n",
       " 'mean_score_time': array([ 0.00628996,  0.00199981,  0.00192728,  0.00189676,  0.00191445,\n",
       "         0.00287113,  0.00287662,  0.00265913,  0.00219293,  0.00196834,\n",
       "         0.0030757 ,  0.0024786 ]),\n",
       " 'mean_test_score': array([ 0.65322581,  0.66935484,  0.66935484,  0.66935484,  0.66935484,\n",
       "         0.64516129,  0.64516129,  0.62903226,  0.62903226,  0.62903226,\n",
       "         0.62903226,  0.62903226]),\n",
       " 'mean_train_score': array([ 0.76612121,  0.79438384,  0.80042424,  0.80038384,  0.80040404,\n",
       "         0.78828283,  0.79030303,  0.79230303,  0.79230303,  0.79230303,\n",
       "         0.79230303,  0.79230303]),\n",
       " 'param_reduce_dim__k': masked_array(data = [1 2 3 4 5 6 7 8 9 10 11 12],\n",
       "              mask = [False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'reduce_dim__k': 1},\n",
       "  {'reduce_dim__k': 2},\n",
       "  {'reduce_dim__k': 3},\n",
       "  {'reduce_dim__k': 4},\n",
       "  {'reduce_dim__k': 5},\n",
       "  {'reduce_dim__k': 6},\n",
       "  {'reduce_dim__k': 7},\n",
       "  {'reduce_dim__k': 8},\n",
       "  {'reduce_dim__k': 9},\n",
       "  {'reduce_dim__k': 10},\n",
       "  {'reduce_dim__k': 11},\n",
       "  {'reduce_dim__k': 12}),\n",
       " 'rank_test_score': array([5, 1, 1, 1, 1, 6, 6, 8, 8, 8, 8, 8], dtype=int32),\n",
       " 'split0_test_score': array([ 0.56,  0.6 ,  0.6 ,  0.6 ,  0.6 ,  0.52,  0.52,  0.52,  0.52,\n",
       "         0.52,  0.52,  0.52]),\n",
       " 'split0_train_score': array([ 0.78787879,  0.81818182,  0.81818182,  0.83838384,  0.83838384,\n",
       "         0.81818182,  0.81818182,  0.81818182,  0.81818182,  0.81818182,\n",
       "         0.81818182,  0.81818182]),\n",
       " 'split1_test_score': array([ 0.72,  0.72,  0.72,  0.68,  0.68,  0.64,  0.64,  0.64,  0.64,\n",
       "         0.64,  0.64,  0.64]),\n",
       " 'split1_train_score': array([ 0.76767677,  0.7979798 ,  0.7979798 ,  0.77777778,  0.78787879,\n",
       "         0.77777778,  0.77777778,  0.77777778,  0.77777778,  0.77777778,\n",
       "         0.77777778,  0.77777778]),\n",
       " 'split2_test_score': array([ 0.64,  0.68,  0.68,  0.68,  0.68,  0.64,  0.64,  0.64,  0.64,\n",
       "         0.64,  0.64,  0.64]),\n",
       " 'split2_train_score': array([ 0.76767677,  0.78787879,  0.80808081,  0.78787879,  0.78787879,\n",
       "         0.75757576,  0.75757576,  0.75757576,  0.75757576,  0.75757576,\n",
       "         0.75757576,  0.75757576]),\n",
       " 'split3_test_score': array([ 0.84,  0.8 ,  0.8 ,  0.84,  0.84,  0.88,  0.88,  0.8 ,  0.8 ,\n",
       "         0.8 ,  0.8 ,  0.8 ]),\n",
       " 'split3_train_score': array([ 0.73737374,  0.78787879,  0.78787879,  0.78787879,  0.78787879,\n",
       "         0.78787879,  0.7979798 ,  0.7979798 ,  0.7979798 ,  0.7979798 ,\n",
       "         0.7979798 ,  0.7979798 ]),\n",
       " 'split4_test_score': array([ 0.5       ,  0.54166667,  0.54166667,  0.54166667,  0.54166667,\n",
       "         0.54166667,  0.54166667,  0.54166667,  0.54166667,  0.54166667,\n",
       "         0.54166667,  0.54166667]),\n",
       " 'split4_train_score': array([ 0.77,  0.78,  0.79,  0.81,  0.8 ,  0.8 ,  0.8 ,  0.81,  0.81,\n",
       "         0.81,  0.81,  0.81]),\n",
       " 'std_fit_time': array([ 0.00873361,  0.00186737,  0.00224843,  0.00243288,  0.00021337,\n",
       "         0.00113914,  0.00082878,  0.00070491,  0.0001079 ,  0.0004235 ,\n",
       "         0.00020093,  0.00291549]),\n",
       " 'std_score_time': array([  4.65199128e-03,   1.82319509e-04,   8.87890251e-05,\n",
       "          1.25454089e-04,   6.20090745e-05,   1.62481943e-03,\n",
       "          2.18176753e-03,   1.15657288e-03,   6.07525485e-04,\n",
       "          2.64395794e-04,   1.17304072e-03,   8.03432096e-04]),\n",
       " 'std_test_score': array([ 0.1194311 ,  0.09003651,  0.09003651,  0.10021006,  0.10021006,\n",
       "         0.12791011,  0.12791011,  0.09907437,  0.09907437,  0.09907437,\n",
       "         0.09907437,  0.09907437]),\n",
       " 'std_train_score': array([ 0.01624619,  0.01319698,  0.01136647,  0.02172674,  0.01956157,\n",
       "         0.02041904,  0.02077569,  0.02205487,  0.02205487,  0.02205487,\n",
       "         0.02205487,  0.02205487])}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   15.6 ,   127.  ,     3.06,     5.64,  1065.  ])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Alcalinity of ash', 'Magnesium', 'Flavanoids', 'Color intensity',\n",
       "       'Proline'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = [4,5,7,10,13]\n",
    "df_wine.columns[z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argwhere() got an unexpected keyword argument 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-8f6288d12484>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: argwhere() got an unexpected keyword argument 'x'"
     ]
    }
   ],
   "source": [
    "np.argwhere(x=X_new[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEQUENTIAL BACKWARD SELECTION\n",
    "\n",
    "**Sequential Backward Selection** (SBS) is a selection algorithm which aims to reduce dimensionality with a minimum decay in performance\n",
    "\n",
    "The idea is:\n",
    "\n",
    "1. Initialize with $k=d$ (where $k$ is the dimensional space objective and $d$ is the dimensionality of the full feature space)\n",
    "2. Determine the feature $x_i$ that gives the worst result (cost function)\n",
    "3. Remove the feature $x_i$\n",
    "4. Terminate if $k$ is reached, if not, go to step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating the class SBS\n",
    "class SBS():\n",
    "    def __init__(self, estimator, k_features, scoring=accuracy_score, test_size=0.25, random_state=1, debug=False):\n",
    "        self.scoring = scoring\n",
    "        self.estimator = clone(estimator)\n",
    "        self.k_features = k_features\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.debug = debug\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                            test_size=self.test_size, \n",
    "                                                            random_state=self.random_state)\n",
    "        dim = X_train.shape[1]\n",
    "        self.indices_ = tuple(range(dim))\n",
    "        self.subsets_ = [self.indices_]\n",
    "        score = self._calc_score(X_train, y_train, X_test, y_test, self.indices_)\n",
    "        self.scores_ = [score]\n",
    "        \n",
    "        if self.debug:\n",
    "            print('Dimensionality: {0} (objective: {1})'.format(dim, self.k_features))\n",
    "            print('Subset: {0}'.format(self.subsets_))\n",
    "            print('Score: {0}'.format(score))\n",
    "            i = 0\n",
    "        \n",
    "        while dim > self.k_features:\n",
    "            scores = []\n",
    "            subsets = []\n",
    "\n",
    "            if self.debug:\n",
    "                print('----------------------')\n",
    "                print('Iteration: {0}'.format(i))\n",
    "                i += 1\n",
    "                j = 1\n",
    "\n",
    "            for p in combinations(self.indices_, r=dim-1):\n",
    "                score = self._calc_score(X_train, y_train, X_test, y_test, p)\n",
    "                scores.append(score)\n",
    "                subsets.append(p)\n",
    "                if self.debug:\n",
    "                    print('Combination {0}: {1}'.format(j, p))\n",
    "                    print('Score: {0}'.format(score))\n",
    "                    j += 1\n",
    "\n",
    "            best = np.argmax(scores)\n",
    "            self.indices_ = subsets[best]\n",
    "            self.subsets_.append(self.indices_)\n",
    "            dim -= 1\n",
    "            \n",
    "            if self.debug:\n",
    "                print('\\nBest Features: {0}'.format(subsets[best]))\n",
    "                print('Best Score: {0}'.format(scores[best]))\n",
    "\n",
    "            self.scores_.append(scores[best])\n",
    "        \n",
    "        self.k_score_ = self.scores_[-1]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[:, self.indices_]\n",
    "        \n",
    "            \n",
    "    def _calc_score(self, X_train, y_train, X_test, y_test, indices):\n",
    "        self.estimator.fit(X_train[:,indices], y_train)\n",
    "        y_pred = self.estimator.predict(X_test[:, indices])\n",
    "        score = self.scoring(y_test, y_pred)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality: 13 (objective: 1)\n",
      "Subset: [(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)]\n",
      "Score: 0.9354838709677419\n",
      "----------------------\n",
      "Iteration: 0\n",
      "Combination 1: (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\n",
      "Score: 0.8709677419354839\n",
      "Combination 2: (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12)\n",
      "Score: 0.9032258064516129\n",
      "Combination 3: (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12)\n",
      "Score: 0.9032258064516129\n",
      "Combination 4: (0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12)\n",
      "Score: 0.9032258064516129\n",
      "Combination 5: (0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12)\n",
      "Score: 0.9354838709677419\n",
      "Combination 6: (0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12)\n",
      "Score: 0.9354838709677419\n",
      "Combination 7: (0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12)\n",
      "Score: 0.9354838709677419\n",
      "Combination 8: (0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12)\n",
      "Score: 0.9354838709677419\n",
      "Combination 9: (0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12)\n",
      "Score: 0.967741935483871\n",
      "Combination 10: (0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12)\n",
      "Score: 0.9354838709677419\n",
      "Combination 11: (0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)\n",
      "Score: 0.967741935483871\n",
      "Combination 12: (0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)\n",
      "Score: 0.9032258064516129\n",
      "Combination 13: (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)\n",
      "Score: 0.8387096774193549\n",
      "\n",
      "Best Features: (0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12)\n",
      "Best Score: 0.967741935483871\n",
      "----------------------\n",
      "Iteration: 1\n",
      "Combination 1: (0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11)\n",
      "Score: 0.9032258064516129\n",
      "Combination 2: (0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 12)\n",
      "Score: 0.967741935483871\n",
      "Combination 3: (0, 1, 2, 3, 5, 6, 7, 8, 9, 11, 12)\n",
      "Score: 0.9354838709677419\n",
      "Combination 4: (0, 1, 2, 3, 5, 6, 7, 8, 10, 11, 12)\n",
      "Score: 0.9354838709677419\n",
      "Combination 5: (0, 1, 2, 3, 5, 6, 7, 9, 10, 11, 12)\n",
      "Score: 0.9032258064516129\n",
      "Combination 6: (0, 1, 2, 3, 5, 6, 8, 9, 10, 11, 12)\n",
      "Score: 0.967741935483871\n",
      "Combination 7: (0, 1, 2, 3, 5, 7, 8, 9, 10, 11, 12)\n",
      "Score: 0.967741935483871\n",
      "Combination 8: (0, 1, 2, 3, 6, 7, 8, 9, 10, 11, 12)\n",
      "Score: 1.0\n",
      "Combination 9: (0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12)\n",
      "Score: 0.9354838709677419\n",
      "Combination 10: (0, 1, 3, 5, 6, 7, 8, 9, 10, 11, 12)\n",
      "Score: 1.0\n",
      "Combination 11: (0, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12)\n",
      "Score: 0.967741935483871\n",
      "Combination 12: (1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12)\n",
      "Score: 0.9354838709677419\n",
      "\n",
      "Best Features: (0, 1, 2, 3, 6, 7, 8, 9, 10, 11, 12)\n",
      "Best Score: 1.0\n",
      "----------------------\n",
      "Iteration: 2\n",
      "Combination 1: (0, 1, 2, 3, 6, 7, 8, 9, 10, 11)\n",
      "Score: 0.9032258064516129\n",
      "Combination 2: (0, 1, 2, 3, 6, 7, 8, 9, 10, 12)\n",
      "Score: 1.0\n",
      "Combination 3: (0, 1, 2, 3, 6, 7, 8, 9, 11, 12)\n",
      "Score: 0.9354838709677419\n",
      "Combination 4: (0, 1, 2, 3, 6, 7, 8, 10, 11, 12)\n",
      "Score: 0.9354838709677419\n",
      "Combination 5: (0, 1, 2, 3, 6, 7, 9, 10, 11, 12)\n",
      "Score: 1.0\n",
      "Combination 6: (0, 1, 2, 3, 6, 8, 9, 10, 11, 12)\n",
      "Score: 0.967741935483871\n",
      "Combination 7: (0, 1, 2, 3, 7, 8, 9, 10, 11, 12)\n",
      "Score: 1.0\n",
      "Combination 8: (0, 1, 2, 6, 7, 8, 9, 10, 11, 12)\n",
      "Score: 0.9354838709677419\n",
      "Combination 9: (0, 1, 3, 6, 7, 8, 9, 10, 11, 12)\n",
      "Score: 1.0\n",
      "Combination 10: (0, 2, 3, 6, 7, 8, 9, 10, 11, 12)\n",
      "Score: 1.0\n",
      "Combination 11: (1, 2, 3, 6, 7, 8, 9, 10, 11, 12)\n",
      "Score: 0.967741935483871\n",
      "\n",
      "Best Features: (0, 1, 2, 3, 6, 7, 8, 9, 10, 12)\n",
      "Best Score: 1.0\n",
      "----------------------\n",
      "Iteration: 3\n",
      "Combination 1: (0, 1, 2, 3, 6, 7, 8, 9, 10)\n",
      "Score: 0.9032258064516129\n",
      "Combination 2: (0, 1, 2, 3, 6, 7, 8, 9, 12)\n",
      "Score: 0.9354838709677419\n",
      "Combination 3: (0, 1, 2, 3, 6, 7, 8, 10, 12)\n",
      "Score: 1.0\n",
      "Combination 4: (0, 1, 2, 3, 6, 7, 9, 10, 12)\n",
      "Score: 1.0\n",
      "Combination 5: (0, 1, 2, 3, 6, 8, 9, 10, 12)\n",
      "Score: 1.0\n",
      "Combination 6: (0, 1, 2, 3, 7, 8, 9, 10, 12)\n",
      "Score: 0.967741935483871\n",
      "Combination 7: (0, 1, 2, 6, 7, 8, 9, 10, 12)\n",
      "Score: 0.9354838709677419\n",
      "Combination 8: (0, 1, 3, 6, 7, 8, 9, 10, 12)\n",
      "Score: 1.0\n",
      "Combination 9: (0, 2, 3, 6, 7, 8, 9, 10, 12)\n",
      "Score: 1.0\n",
      "Combination 10: (1, 2, 3, 6, 7, 8, 9, 10, 12)\n",
      "Score: 0.9032258064516129\n",
      "\n",
      "Best Features: (0, 1, 2, 3, 6, 7, 8, 10, 12)\n",
      "Best Score: 1.0\n",
      "----------------------\n",
      "Iteration: 4\n",
      "Combination 1: (0, 1, 2, 3, 6, 7, 8, 10)\n",
      "Score: 0.9032258064516129\n",
      "Combination 2: (0, 1, 2, 3, 6, 7, 8, 12)\n",
      "Score: 0.9354838709677419\n",
      "Combination 3: (0, 1, 2, 3, 6, 7, 10, 12)\n",
      "Score: 0.967741935483871\n",
      "Combination 4: (0, 1, 2, 3, 6, 8, 10, 12)\n",
      "Score: 1.0\n",
      "Combination 5: (0, 1, 2, 3, 7, 8, 10, 12)\n",
      "Score: 0.967741935483871\n",
      "Combination 6: (0, 1, 2, 6, 7, 8, 10, 12)\n",
      "Score: 0.967741935483871\n",
      "Combination 7: (0, 1, 3, 6, 7, 8, 10, 12)\n",
      "Score: 1.0\n",
      "Combination 8: (0, 2, 3, 6, 7, 8, 10, 12)\n",
      "Score: 1.0\n",
      "Combination 9: (1, 2, 3, 6, 7, 8, 10, 12)\n",
      "Score: 0.8709677419354839\n",
      "\n",
      "Best Features: (0, 1, 2, 3, 6, 8, 10, 12)\n",
      "Best Score: 1.0\n",
      "----------------------\n",
      "Iteration: 5\n",
      "Combination 1: (0, 1, 2, 3, 6, 8, 10)\n",
      "Score: 0.9354838709677419\n",
      "Combination 2: (0, 1, 2, 3, 6, 8, 12)\n",
      "Score: 0.9354838709677419\n",
      "Combination 3: (0, 1, 2, 3, 6, 10, 12)\n",
      "Score: 0.967741935483871\n",
      "Combination 4: (0, 1, 2, 3, 8, 10, 12)\n",
      "Score: 0.9354838709677419\n",
      "Combination 5: (0, 1, 2, 6, 8, 10, 12)\n",
      "Score: 0.9354838709677419\n",
      "Combination 6: (0, 1, 3, 6, 8, 10, 12)\n",
      "Score: 1.0\n",
      "Combination 7: (0, 2, 3, 6, 8, 10, 12)\n",
      "Score: 0.967741935483871\n",
      "Combination 8: (1, 2, 3, 6, 8, 10, 12)\n",
      "Score: 0.9354838709677419\n",
      "\n",
      "Best Features: (0, 1, 3, 6, 8, 10, 12)\n",
      "Best Score: 1.0\n",
      "----------------------\n",
      "Iteration: 6\n",
      "Combination 1: (0, 1, 3, 6, 8, 10)\n",
      "Score: 0.9032258064516129\n",
      "Combination 2: (0, 1, 3, 6, 8, 12)\n",
      "Score: 0.967741935483871\n",
      "Combination 3: (0, 1, 3, 6, 10, 12)\n",
      "Score: 1.0\n",
      "Combination 4: (0, 1, 3, 8, 10, 12)\n",
      "Score: 0.967741935483871\n",
      "Combination 5: (0, 1, 6, 8, 10, 12)\n",
      "Score: 0.967741935483871\n",
      "Combination 6: (0, 3, 6, 8, 10, 12)\n",
      "Score: 0.967741935483871\n",
      "Combination 7: (1, 3, 6, 8, 10, 12)\n",
      "Score: 1.0\n",
      "\n",
      "Best Features: (0, 1, 3, 6, 10, 12)\n",
      "Best Score: 1.0\n",
      "----------------------\n",
      "Iteration: 7\n",
      "Combination 1: (0, 1, 3, 6, 10)\n",
      "Score: 0.9354838709677419\n",
      "Combination 2: (0, 1, 3, 6, 12)\n",
      "Score: 0.9354838709677419\n",
      "Combination 3: (0, 1, 3, 10, 12)\n",
      "Score: 1.0\n",
      "Combination 4: (0, 1, 6, 10, 12)\n",
      "Score: 0.967741935483871\n",
      "Combination 5: (0, 3, 6, 10, 12)\n",
      "Score: 0.967741935483871\n",
      "Combination 6: (1, 3, 6, 10, 12)\n",
      "Score: 0.967741935483871\n",
      "\n",
      "Best Features: (0, 1, 3, 10, 12)\n",
      "Best Score: 1.0\n",
      "----------------------\n",
      "Iteration: 8\n",
      "Combination 1: (0, 1, 3, 10)\n",
      "Score: 0.9032258064516129\n",
      "Combination 2: (0, 1, 3, 12)\n",
      "Score: 0.9032258064516129\n",
      "Combination 3: (0, 1, 10, 12)\n",
      "Score: 0.9354838709677419\n",
      "Combination 4: (0, 3, 10, 12)\n",
      "Score: 0.9354838709677419\n",
      "Combination 5: (1, 3, 10, 12)\n",
      "Score: 0.9354838709677419\n",
      "\n",
      "Best Features: (0, 1, 10, 12)\n",
      "Best Score: 0.9354838709677419\n",
      "----------------------\n",
      "Iteration: 9\n",
      "Combination 1: (0, 1, 10)\n",
      "Score: 0.967741935483871\n",
      "Combination 2: (0, 1, 12)\n",
      "Score: 0.8709677419354839\n",
      "Combination 3: (0, 10, 12)\n",
      "Score: 0.9354838709677419\n",
      "Combination 4: (1, 10, 12)\n",
      "Score: 0.9354838709677419\n",
      "\n",
      "Best Features: (0, 1, 10)\n",
      "Best Score: 0.967741935483871\n",
      "----------------------\n",
      "Iteration: 10\n",
      "Combination 1: (0, 1)\n",
      "Score: 0.7419354838709677\n",
      "Combination 2: (0, 10)\n",
      "Score: 0.8709677419354839\n",
      "Combination 3: (1, 10)\n",
      "Score: 0.7096774193548387\n",
      "\n",
      "Best Features: (0, 10)\n",
      "Best Score: 0.8709677419354839\n",
      "----------------------\n",
      "Iteration: 11\n",
      "Combination 1: (0,)\n",
      "Score: 0.8387096774193549\n",
      "Combination 2: (10,)\n",
      "Score: 0.6451612903225806\n",
      "\n",
      "Best Features: (0,)\n",
      "Best Score: 0.8387096774193549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.SBS at 0x119654080>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plots configuration\n",
    "#Â plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = 12, 8\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['legend.fontsize'] = 11\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "sbs = SBS(estimator=knn, k_features=1, debug=True)\n",
    "sbs.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAt0AAAHnCAYAAABkPuGkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbfZfavRoKSE7l/kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119649cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_feat = [len(k) for k in sbs.subsets_]\n",
    "plt.plot(k_feat, sbs.scores_, marker='o')\n",
    "plt.ylim([0.7, 1.1])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('number of features')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Alcohol', 'Malic acid', 'Alcalinity of ash', 'Hue', 'Proline'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "k5 = list(sbs.subsets_[8])\n",
    "print(df_wine.columns[1:][k5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12),\n",
       " (0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12),\n",
       " (0, 1, 2, 3, 6, 7, 8, 9, 10, 11, 12),\n",
       " (0, 1, 2, 3, 6, 7, 8, 9, 10, 12),\n",
       " (0, 1, 2, 3, 6, 7, 8, 10, 12),\n",
       " (0, 1, 2, 3, 6, 8, 10, 12),\n",
       " (0, 1, 3, 6, 8, 10, 12),\n",
       " (0, 1, 3, 6, 10, 12),\n",
       " (0, 1, 3, 10, 12),\n",
       " (0, 1, 10, 12),\n",
       " (0, 1, 10),\n",
       " (0, 10),\n",
       " (0,)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbs.subsets_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the performance of the KNN Classifier on the original test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9838709677419355\n",
      "Test accuracy: 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "knn.fit(X_train_scaled, y_train)\n",
    "print('Training accuracy: {0}'.format(knn.score(X_train_scaled, y_train)))\n",
    "print('Test accuracy: {0}'.format(knn.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding cell we used the complete feature set and the accuracy in the training and test datasets are slightly different showing a slight degree of **overfitting** (since the accuracy of the training set is higher than the accuracy of the test set) \n",
    "\n",
    "Now, let's use the selected 5-feature subset and see how well KNN performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9596774193548387\n",
      "Test accuracy: 0.9629629629629629\n"
     ]
    }
   ],
   "source": [
    "knn.fit(X_train_scaled[:, k5], y_train)\n",
    "print('Training accuracy: {0}'.format(knn.score(X_train_scaled[:, k5], y_train)))\n",
    "print('Test accuracy: {0}'.format(knn.score(X_test_scaled[:, k5], y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using fewer than half of the original features, the prediction accuracy on the test set improved by almost 2 percent. Also, we reduced overfitting which we can tell from the small gap between test and training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dspy3]",
   "language": "python",
   "name": "conda-env-dspy3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
